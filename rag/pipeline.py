"""RAG query LLM."""
from vectorstore.chromadb_store import ChromaStore
import sys
import time
import logging
import hashlib
from typing import List, Dict, Any, Optional, Tuple
from settings import LLM_MODEL, OLLAMA_BASE, SUMM_K, CHUNKS_K, MAX_CTX_CHARS, MAX_CHUNK_CHARS
from rag.llm_backends import OllamaBackend, DeepSeekBackend, OpenAIBackend, GeminiBackend
from settings import RAG_SYSTEM_PROMPT, RAG_ANSWER_STYLE
from transformers import pipeline
import numpy as np
from rank_bm25 import BM25Okapi
from utils.keywords import extract_keywords, string_to_keywords
from utils.metrics import track_time, PerformanceMetrics

sys.path.insert(0, '.')  # settings/ChromaStore

logger = logging.getLogger(__name__)

class RAGPipeline:
    def __init__(self, backend: str = "local"):
        self.store = ChromaStore()
        if backend == "local":
            self.llm = OllamaBackend()
        elif backend == "deepseek":
            self.llm = DeepSeekBackend()
        elif backend == "openai":
            self.llm = OpenAIBackend()
        elif backend == "gemini":
            self.llm = GeminiBackend()
        else:
            raise ValueError(f"Unknown LLM backend: {backend}")
        # –õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ classifier - –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏
        self._classifier = None
        # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ BM25 –∏–Ω–¥–µ–∫—Å–∞
        self.bm25_index = None
        self.bm25_corpus = None
        self.bm25_corpus_hash = None
    
    @property
    def classifier(self):
        """–õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ classifier –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –æ–±—Ä–∞—â–µ–Ω–∏–∏."""
        if self._classifier is None:
            logger.debug("[RAG] –ó–∞–≥—Ä—É–∑–∫–∞ classifier...")
            self._classifier = pipeline("zero-shot-classification",
                                       model="typeform/distilbert-base-uncased-mnli",  # 29MB –≤–º–µ—Å—Ç–æ 714MB
                                       device=-1)
            logger.debug("[RAG] Classifier –∑–∞–≥—Ä—É–∂–µ–Ω")
        return self._classifier

    def _filter_by_keywords(self, results: Dict[str, Any], question_keywords: List[str]) -> Dict[str, Any]:
        """
        Keyword-based –ø–æ—Å—Ç—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ—Å–ª–µ semantic search.
        –§–∏–ª—å—Ç—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ keywords –≤ metadata.
        –í—Ä–µ–º—è: ~0.001s
        
        Args:
            results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∑–∞–ø—Ä–æ—Å–∞ –∏–∑ ChromaDB
            question_keywords: –°–ø–∏—Å–æ–∫ keywords –∏–∑ –≤–æ–ø—Ä–æ—Å–∞
            
        Returns:
            –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        """
        if not question_keywords or not results.get("metadatas") or not results["metadatas"][0]:
            return results
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ keywords
        filtered_metas = []
        filtered_docs = []
        filtered_dists = []
        
        for meta, doc, dist in zip(
            results["metadatas"][0],
            results["documents"][0],
            results["distances"][0]
        ):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ keywords –≤ metadata —á–∞–Ω–∫–∞
            chunk_keywords_str = meta.get("keywords", "")
            if not chunk_keywords_str:
                # –ï—Å–ª–∏ keywords –Ω–µ—Ç, –æ—Å—Ç–∞–≤–ª—è–µ–º —á–∞–Ω–∫ (–æ–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å)
                filtered_metas.append(meta)
                filtered_docs.append(doc)
                filtered_dists.append(dist)
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ–≥–æ keyword
            chunk_keywords = [kw.strip().lower() for kw in chunk_keywords_str.split(",")]
            question_keywords_lower = [kw.lower() for kw in question_keywords if len(kw) >= 3]
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ, –æ—Å—Ç–∞–≤–ª—è–µ–º —á–∞–Ω–∫
            if any(qkw in chunk_keywords for qkw in question_keywords_lower):
                filtered_metas.append(meta)
                filtered_docs.append(doc)
                filtered_dists.append(dist)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        return {
            "metadatas": [filtered_metas] if filtered_metas else [[]],
            "documents": [filtered_docs] if filtered_docs else [[]],
            "distances": [filtered_dists] if filtered_dists else [[]],
        }

    def query(
            self,
            question,
            source: str | None = None,
            channel: str | None = None,
            channels: List[str] | None = None,  # –°–ø–∏—Å–æ–∫ –∫–∞–Ω–∞–ª–æ–≤ –¥–ª—è –º—É–ª—å—Ç–∏–∫–∞–Ω–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
            extra_year_summaries: dict[int, str] | None = None,
            extra_quarter_summaries: dict[tuple[int, int], str] | None = None,
    ):

        start_total = time.time()  # üü¢ TOTAL START
        
        logger.info(f"[RAG] ========== –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞ ==========")
        logger.info(f"[RAG] –í–æ–ø—Ä–æ—Å: {question[:100]}..." if len(question) > 100 else f"[RAG] –í–æ–ø—Ä–æ—Å: {question}")
        if channels:
            logger.info(f"[RAG] –ö–∞–Ω–∞–ª—ã: {', '.join(channels)} (–º—É–ª—å—Ç–∏–∫–∞–Ω–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫)")
        else:
            logger.info(f"[RAG] –ö–∞–Ω–∞–ª: {channel or '–≤—Å–µ –∫–∞–Ω–∞–ª—ã'}")
        logger.info(f"[RAG] Backend: {self.llm.__class__.__name__}")

        t1 = time.time()
        query_text = f"query: {question}"
        logger.debug(f"[RAG] –°–æ–∑–¥–∞–Ω–∏–µ embedding –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞...")
        emb = self.store.model.encode(query_text).tolist()
        embedding_time = time.time() - t1
        logger.info(f"[RAG] Embedding —Å–æ–∑–¥–∞–Ω –∑–∞ {embedding_time:.2f}s")

        # --- 2) –¥–≤–∞ –∑–∞–ø—Ä–æ—Å–∞ –≤ Chroma: —Å–∞–º–º–∞—Ä–∏ –∏ —Å—ã—Ä—ã–µ —á–∞–Ω–∫–∏ ---
        t2 = time.time()

        where_base: dict | None = None
        # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –º—É–ª—å—Ç–∏–∫–∞–Ω–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        if channels and len(channels) > 0:
            # –§–∏–ª—å—Ç—Ä –ø–æ —Å–ø–∏—Å–∫—É –∫–∞–Ω–∞–ª–æ–≤: channel IN [ch1, ch2, ...]
            where_base = {
                "$or": [{"channel": {"$eq": ch}} for ch in channels]
            }
            if source:
                where_base = {
                    "$and": [
                        {"source": {"$eq": source}},
                        {"$or": [{"channel": {"$eq": ch}} for ch in channels]}
                    ]
                }
        elif source and channel:
            where_base = {
                "$and": [
                    {"source": {"$eq": source}},
                    {"channel": {"$eq": channel}},
                ]
            }
        elif source:
            where_base = {"source": {"$eq": source}}
        elif channel:
            where_base = {"channel": {"$eq": channel}}

        # 2.1 –°–∞–º–º–∞—Ä–∏
        if channels and len(channels) > 0:
            # –ú—É–ª—å—Ç–∏–∫–∞–Ω–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫: —Å–∞–º–º–∞—Ä–∏ –∏–∑ –≤—Å–µ—Ö –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤
            where_summ = {
                "$and": [
                    {"type": {"$eq": "summary"}},
                    {"$or": [{"channel": {"$eq": ch}} for ch in channels]}
                ]
            }
        elif channel:
            where_summ = {
                "$and": [
                    {"type": {"$eq": "summary"}},
                    {"channel": {"$eq": channel}},
                ]
            }
        else:
            # –í—Å–µ –∫–∞–Ω–∞–ª—ã
            where_summ = {"type": {"$eq": "summary"}}
        res_summ = self.store.collection.query(
            query_embeddings=[emb],
            n_results=SUMM_K,
            include=["metadatas", "documents", "distances"],
            where=where_summ,
        )

        # 2.2 –°—ã—Ä—ã–µ —á–∞–Ω–∫–∏
        where_chunks = {"type": {"$ne": "summary"}}
        if where_base:
            where_chunks = {"$and": [where_chunks, where_base]}

        logger.info(f"[RAG] –ü–æ–∏—Å–∫ –≤ ChromaDB: —Å–∞–º–º–∞—Ä–∏ (K={SUMM_K}) –∏ —á–∞–Ω–∫–∏ (K={CHUNKS_K})...")
        res_chunks = self.store.collection.query(
            query_embeddings=[emb],
            n_results=CHUNKS_K,
            include=["metadatas", "documents", "distances"],
            where=where_chunks,
        )
        
        # –ù–û–í–û–ï: Keyword-based –ø–æ—Å—Ç—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è (–ø–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤)
        # –ò–∑–≤–ª–µ–∫–∞–µ–º keywords –∏–∑ –≤–æ–ø—Ä–æ—Å–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        question_keywords = extract_keywords(question, top_k=5, use_keybert=False)
        logger.debug(f"[RAG] –ò–∑–≤–ª–µ—á–µ–Ω–æ keywords –∏–∑ –≤–æ–ø—Ä–æ—Å–∞: {question_keywords}")
        
        if question_keywords:
            logger.debug(f"[RAG] –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ keyword-—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º...")
            before_count = len(res_chunks.get('metadatas', [[]])[0]) if res_chunks.get('metadatas') else 0
            filtered_chunks = self._filter_by_keywords(res_chunks, question_keywords)
            after_count = len(filtered_chunks.get('metadatas', [[]])[0]) if filtered_chunks.get('metadatas') else 0
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å–ª–∏—à–∫–æ–º –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è (–æ—Å—Ç–∞–ª–æ—Å—å >= 30% —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤)
            # –≠—Ç–æ –∑–∞—â–∏—â–∞–µ—Ç –æ—Ç —Å–ª—É—á–∞–µ–≤, –∫–æ–≥–¥–∞ –≤—Å–µ —á–∞–Ω–∫–∏ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤—ã–≤–∞—é—Ç—Å—è
            if before_count > 0 and after_count >= max(1, before_count * 0.3):
                res_chunks = filtered_chunks
                logger.info(f"[RAG] Keyword-—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∞: {before_count} -> {after_count} —á–∞–Ω–∫–æ–≤")
            else:
                logger.warning(f"[RAG] Keyword-—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å–ª–∏—à–∫–æ–º –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è ({before_count} -> {after_count}), –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                logger.info(f"[RAG] –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ {before_count} —á–∞–Ω–∫–æ–≤ –±–µ–∑ keyword-—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏")
        
        chroma_time = time.time() - t2
        logger.info(f"[RAG] ChromaDB –∑–∞–ø—Ä–æ—Å –≤—ã–ø–æ–ª–Ω–µ–Ω –∑–∞ {chroma_time:.2f}s")
        logger.info(f"[RAG] –ù–∞–π–¥–µ–Ω–æ —Å–∞–º–º–∞—Ä–∏: {len(res_summ.get('metadatas', [[]])[0]) if res_summ.get('metadatas') else 0}")
        logger.info(f"[RAG] –ù–∞–π–¥–µ–Ω–æ —á–∞–Ω–∫–æ–≤: {len(res_chunks.get('metadatas', [[]])[0]) if res_chunks.get('metadatas') else 0}")

        # --- 3) —Ä–∞–∑–¥–µ–ª—å–Ω—ã–π —Ä–µ—Ä–∞–Ω–∫ ---
        t3 = time.time()
        logger.info(f"[RAG] –ù–∞—á–∞–ª–æ reranking...")
        question_words = set(question.lower().split())
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ keywords –¥–ª—è BM25 (–∏–∑–≤–ª–µ—á–µ–Ω—ã –≤—ã—à–µ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏)
        logger.debug(f"[RAG] Keywords –¥–ª—è BM25: {question_keywords}")

        def rerank(res: Dict[str, Any]) -> List[Tuple[float, str, Dict[str, Any]]]:
            """Rerank —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º BM25 –∏ keyword matching."""
            scored: List[Tuple[float, str, Dict[str, Any]]] = []
            if not res["metadatas"] or not res["metadatas"][0]:
                return scored

            metas = res["metadatas"][0]
            docs = res["documents"][0]
            dists = res["distances"][0]

            # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ BM25 –∏–Ω–¥–µ–∫—Å–∞: —Å–æ–∑–¥–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑–º–µ–Ω–∏–ª–∏—Å—å
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ö–µ—à —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π
            docs_str = "|".join(docs)
            docs_hash = hashlib.md5(docs_str.encode()).hexdigest()
            
            if self.bm25_corpus_hash != docs_hash:
                logger.debug(f"[RAG] –°–æ–∑–¥–∞–Ω–∏–µ BM25 –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
                # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è BM25
                tokenized_docs = [doc.lower().split() for doc in docs]
                self.bm25_index = BM25Okapi(tokenized_docs)
                self.bm25_corpus = tokenized_docs
                self.bm25_corpus_hash = docs_hash
                logger.debug(f"[RAG] BM25 –∏–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω –∏ –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω")
            else:
                logger.debug(f"[RAG] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ BM25 –∏–Ω–¥–µ–∫—Å–∞")

            for i, (meta, doc, dist) in enumerate(zip(metas, docs, dists)):
                doc_lower = doc.lower()
                channel = meta.get("channel", "").lower()

                sem_score = -dist

                # –ù–û–í–û–ï: BM25 scoring –¥–ª—è keywords
                bm25_score = 0
                if question_keywords and self.bm25_index:
                    tokenized_query = [kw for kw in question_keywords if len(kw) >= 3]
                    if tokenized_query:
                        bm25_scores = self.bm25_index.get_scores(tokenized_query)
                        bm25_score = bm25_scores[i] if i < len(bm25_scores) else 0
                
                # –°—Ç–∞—Ä—ã–π keyword matching (–æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ fallback, –Ω–æ —Å –º–µ–Ω—å—à–∏–º –≤–µ—Å–æ–º)
                kw_hits = 0
                for w in question_words:
                    if len(w) < 3:
                        continue
                    if w in doc_lower or w in channel:
                        kw_hits += 1
                kw_score = kw_hits * 0.5  # –£–º–µ–Ω—å—à–∞–µ–º –≤–µ—Å —Å—Ç–∞—Ä–æ–≥–æ –º–µ—Ç–æ–¥–∞

                # boost –¥–ª—è —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ –∫–∞–Ω–∞–ª–∞
                channel_boost = 1.0
                if channel and any(w in channel for w in question_words if len(w) > 3):
                    channel_boost = 1.3

                # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º: semantic + BM25 + —Å—Ç–∞—Ä—ã–π keyword
                total_score = sem_score + bm25_score * 2.0 + kw_score * channel_boost
                scored.append((total_score, doc, meta))

            scored.sort(key=lambda x: x[0], reverse=True)
            return scored
        def mmr_select(
            scored: List[Tuple[float, str, Dict[str, Any]]],
            emb_query: List[float],
            doc_embs_precomputed: Optional[np.ndarray] = None,
            top_n: int = 15,
            lambda_mult: float = 0.8
        ) -> List[Tuple[float, str, Dict[str, Any]]]:
            """
            scored: [(score, doc, meta), ...] –ø–æ—Å–ª–µ rerank
            emb_query: –≤–µ–∫—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–∞ (list/np.array)
            doc_embs_precomputed: –ø—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã–µ embeddings (–µ—Å–ª–∏ –µ—Å—Ç—å, –Ω–µ –∫–æ–¥–∏—Ä—É–µ–º –ø–æ–≤—Ç–æ—Ä–Ω–æ)
            """
            if not scored:
                return []

            # –ï—Å–ª–∏ embeddings —É–∂–µ –≤—ã—á–∏—Å–ª–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö (–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è)
            if doc_embs_precomputed is not None:
                doc_embs = np.array(doc_embs_precomputed)
            else:
                # Fallback: –∫–æ–¥–∏—Ä—É–µ–º –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω—ã
                docs = [doc for _, doc, _ in scored]
                doc_embs = np.array(self.store.model.encode(docs))

            q = np.array(emb_query)

            # –Ω–æ—Ä–º–∏—Ä—É–µ–º
            q = q / (np.linalg.norm(q) + 1e-8)
            doc_embs = doc_embs / (np.linalg.norm(doc_embs, axis=1, keepdims=True) + 1e-8)

            # –∫–æ—Å–∏–Ω—É—Å–Ω–∞—è –±–ª–∏–∑–æ—Å—Ç—å
            rel = doc_embs @ q  # [len]

            selected_idx = []
            remaining_idx = list(range(len(scored)))

            while remaining_idx and len(selected_idx) < top_n:
                if not selected_idx:
                    # –ø–µ—Ä–≤—ã–π ‚Äî –ø—Ä–æ—Å—Ç–æ —Å–∞–º—ã–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π
                    best_i = int(np.argmax(rel[remaining_idx]))
                    idx = remaining_idx[best_i]
                    selected_idx.append(idx)
                    remaining_idx.remove(idx)
                    continue

                # –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ —Å—á–∏—Ç–∞–µ–º max similarity —Å —É–∂–µ –≤—ã–±—Ä–∞–Ω–Ω—ã–º–∏
                selected_vecs = doc_embs[selected_idx]  # [k, dim]
                sims_to_selected = doc_embs[remaining_idx] @ selected_vecs.T  # [m, k]
                max_sims = sims_to_selected.max(axis=1)  # [m]

                mmr_scores = (
                    lambda_mult * rel[remaining_idx] - (1 - lambda_mult) * max_sims
                )
                best_i = int(np.argmax(mmr_scores))
                idx = remaining_idx[best_i]
                selected_idx.append(idx)
                remaining_idx.remove(idx)

            # –≤–µ—Ä–Ω—É—Ç—å –≤ —Ç–æ–º –∂–µ —Ñ–æ—Ä–º–∞—Ç–µ [(score, doc, meta)]
            return [scored[i] for i in selected_idx]

        scored_summaries = rerank(res_summ)
        scored_chunks = rerank(res_chunks)
        rerank_time = time.time() - t3
        logger.info(f"[RAG] Reranking –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {rerank_time:.2f}s")
        logger.info(f"[RAG] –û—Ç—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–æ —Å–∞–º–º–∞—Ä–∏: {len(scored_summaries)}, —á–∞–Ω–∫–æ–≤: {len(scored_chunks)}")

        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ö–æ–¥–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –û–î–ò–ù —Ä–∞–∑ –∏ –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–ª—è MMR
        logger.info(f"[RAG] –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ Smart MMR –¥–ª—è –¥–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
        docs_for_mmr = [doc for _, doc, _ in scored_chunks]
        doc_embs_for_mmr = self.store.model.encode(docs_for_mmr)  # –û–î–ò–ù —Ä–∞–∑
        candidates_embs = np.array(doc_embs_for_mmr)

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–ª—è smart_mmr_lambda
        lambda_mmr = self.smart_mmr_lambda(question, candidates_embs)
        
        # –ü–µ—Ä–µ–¥–∞–µ–º embeddings –≤ mmr_select, —á—Ç–æ–±—ã –Ω–µ –∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –ø–æ–≤—Ç–æ—Ä–Ω–æ
        scored_chunks = mmr_select(scored_chunks, emb, doc_embs_for_mmr, top_n=15, lambda_mult=lambda_mmr)
        logger.info(f"[RAG] Smart MMR –∑–∞–≤–µ—Ä—à–µ–Ω (Œª={lambda_mmr:.2f}), –≤—ã–±—Ä–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {len(scored_chunks)}")

        # --- 4) —Å–±–æ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: —Å–Ω–∞—á–∞–ª–∞ —Å–∞–º–º–∞—Ä–∏, –ø–æ—Ç–æ–º —á–∞–Ω–∫–∏ ---
        t4 = time.time()
        logger.info(f"[RAG] –°–±–æ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ —Å–∞–º–º–∞—Ä–∏ –∏ —á–∞–Ω–∫–æ–≤...")
        context_parts = []

        MAX_SUMM_IN_CTX = 6
        for score, doc, meta in scored_summaries[:MAX_SUMM_IN_CTX]:
            ch = meta.get("channel", channel or "unknown")
            period = meta.get("period", meta.get("year", ""))
            header = f"–°–ê–ú–ú–ê–†–ò: {ch}"
            if period:
                header += f" | –ü–ï–†–ò–û–î: {period}"
            context_parts.append(f"{header}\n{doc.strip()}")

        MAX_CHUNKS_IN_CTX = 15
        
        def short_date(d: str) -> str:
            """–ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —á–∞—Å—Ç—å 'YYYY-MM-DD' –∏–∑ ISO –¥–∞—Ç—ã."""
            return d.split("T", 1)[0] if "T" in d else d
        
        for i, (score, doc, meta) in enumerate(scored_chunks[:MAX_CHUNKS_IN_CTX]):
            chan = meta.get("channel")
            start_ts = meta.get("start_ts")
            end_ts = meta.get("end_ts")
            msg_dates = meta.get("msg_dates")
            is_forwarded = meta.get("is_forwarded", False)
            forwarded_from = meta.get("forwarded_from")

            header = f"–ö–ê–ù–ê–õ: {chan}" if chan else "–ö–ê–ù–ê–õ: unknown"
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–º–µ—Ç–∫—É –æ –ø–µ—Ä–µ—Å—ã–ª–∫–µ
            if is_forwarded and forwarded_from:
                header += f" | –ü–ï–†–ï–°–´–õ–ö–ê –∏–∑ {forwarded_from}"
            
            if msg_dates:
                dates = msg_dates.split(",")
                start_d = short_date(dates[0])
                end_d = short_date(dates[-1])
                header += f" | –°–û–û–ë–©–ï–ù–ò–Ø: {start_d} ‚Äî {end_d}"
            elif start_ts and end_ts:
                header += f" | –ü–ï–†–ò–û–î: {short_date(start_ts)} ‚Äî {short_date(end_ts)}"
            elif start_ts:
                header += f" | –î–ê–¢–ê: {short_date(start_ts)}"

            # –ú—è–≥–∫–∞—è –æ–±—Ä–µ–∑–∫–∞: —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —á–∞–Ω–∫ –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–π
            doc_trimmed = doc[:MAX_CHUNK_CHARS] + "..." if len(doc) > MAX_CHUNK_CHARS else doc
            context_parts.append(f"{header}\n{doc_trimmed.strip()}")

        context = "\n\n".join(context_parts)

        if len(context) > MAX_CTX_CHARS:
            context = context[-MAX_CTX_CHARS:]

        # –∞–≥—Ä–µ–≥–∞—Ç—ã –ø–æ —á–∞–Ω–∫–∞–º
        total_chunks = len(context_parts)
        chunk_lens = [len(part) for part in context_parts]
        total_chunk_chars = sum(chunk_lens)

        context_time = time.time() - t4
        logger.info(f"[RAG] –ö–æ–Ω—Ç–µ–∫—Å—Ç —Å–æ–±—Ä–∞–Ω –∑–∞ {context_time:.2f}s")
        logger.info(f"[RAG] –ö–æ–Ω—Ç–µ–∫—Å—Ç: {total_chunks} —á–∞—Å—Ç–µ–π, {len(context)} —Å–∏–º–≤–æ–ª–æ–≤ (~{len(context) // 4} —Ç–æ–∫–µ–Ω–æ–≤)")
        logger.debug(f"[RAG] –ò—Å—Ç–æ—á–Ω–∏–∫–∏ (—Å–∞–º–º–∞—Ä–∏): {[s[2].get('period', s[2].get('doc_id', 'unk')) for s in scored_summaries[:4]]}")
        logger.debug(f"[RAG] –ò—Å—Ç–æ—á–Ω–∏–∫–∏ (—á–∞–Ω–∫–∏): {[c[2].get('msg_dates', 'unk')[:20] for c in scored_chunks[:4]]}")

        if not context.strip():
            total_time = time.time() - start_total
            logger.warning(f"[RAG] –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø—É—Å—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ")
            logger.info(f"[RAG] ========== –ó–∞–ø—Ä–æ—Å –∑–∞–≤–µ—Ä—à–µ–Ω (–Ω–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞) –∑–∞ {total_time:.2f}s ==========")
            return "–ù–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞."

        t5 = time.time()
        logger.info(f"[RAG] –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è LLM...")
        prompt = f"""–°–ò–°–¢–ï–ú–ê:
        {RAG_SYSTEM_PROMPT}
        –ö–û–ù–¢–ï–ö–°–¢:
        {context}
        –ò–ù–°–¢–†–£–ö–¶–ò–Ø:
        {RAG_ANSWER_STYLE}
        –í–û–ü–†–û–°:
        {question}
        –û–¢–í–ï–¢:
        """
        logger.debug(f"[RAG] –ü—Ä–æ–º–ø—Ç —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω, –¥–ª–∏–Ω–∞: {len(prompt)} —Å–∏–º–≤–æ–ª–æ–≤")

        logger.info(f"[RAG] –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –≤ LLM ({self.llm.__class__.__name__})...")
        try:
            from exceptions import LLMError
            answer = self.llm.generate(prompt)
            llm_time = time.time() - t5
            logger.info(f"[RAG] LLM –æ—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –∑–∞ {llm_time:.2f}s")
            logger.debug(f"[RAG] –î–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞: {len(answer)} —Å–∏–º–≤–æ–ª–æ–≤")
        except Exception as e:
            llm_time = time.time() - t5
            logger.error(f"[RAG] –û—à–∏–±–∫–∞ LLM: {e}", exc_info=True)
            # Graceful degradation: –ø—Ä–æ–±—É–µ–º fallback –Ω–∞ Ollama –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
            try:
                logger.warning(f"[RAG] –ü—Ä–æ–±—É–µ–º fallback –Ω–∞ Ollama...")
                fallback_llm = OllamaBackend()
                answer = fallback_llm.generate(prompt)
                logger.info(f"[RAG] Fallback —É—Å–ø–µ—à–µ–Ω, –æ—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –æ—Ç Ollama")
            except Exception as fallback_error:
                logger.error(f"[RAG] Fallback —Ç–∞–∫–∂–µ –Ω–µ —É–¥–∞–ª—Å—è: {fallback_error}")
                from exceptions import LLMError
                raise LLMError(
                    "–°–µ—Ä–≤–∏—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. "
                    "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç."
                ) from fallback_error

        try:
            from utils.source_links import filter_sources_strict, format_sources_section
            
            filtered_sources = filter_sources_strict(
                scored_chunks[:MAX_CHUNKS_IN_CTX],
                max_sources=7
            )
            
            if filtered_sources:
                sources_section = format_sources_section(filtered_sources, use_html=True)
                answer = f"{answer}{sources_section}"
                logger.debug(f"[RAG] –î–æ–±–∞–≤–ª–µ–Ω–æ {len(filtered_sources)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –≤ –∫–æ–Ω–µ—Ü –æ—Ç–≤–µ—Ç–∞")
        except Exception as e:
            logger.warning(f"[RAG] –û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {e}", exc_info=True)

        total_time = time.time() - start_total
        logger.info(f"[RAG] ========== –ó–∞–ø—Ä–æ—Å —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∑–∞ {total_time:.2f}s ==========")
        logger.info(f"[RAG] –í—Ä–µ–º—è –ø–æ —ç—Ç–∞–ø–∞–º: embedding={embedding_time:.2f}s, chroma={chroma_time:.2f}s, rerank={rerank_time:.2f}s, context={context_time:.2f}s, llm={llm_time:.2f}s")
        return answer

    def smart_mmr_lambda(self, question: str, candidates_embs: np.array) -> float:
        """–ì–∏–±—Ä–∏–¥: RuBERT-Tiny (0.02s) + Dynamic Density (0.01s)"""
        import numpy as np
        from sklearn.metrics.pairwise import cosine_similarity

        # 1. üü¢ RuBERT-Tiny –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä (0.02s)
        result = self.classifier(question,
                                 ["—Å–ø–∏—Å–æ–∫ —Ä–∞–∑–Ω—ã—Ö –ø—Ä–æ–µ–∫—Ç–æ–≤", "–¥–µ—Ç–∞–ª–∏ –æ–¥–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞"])
        rubert_score = result['scores'][0]  # 0.92 = —Å–ø–∏—Å–æ–∫
        rubert_type = 0.15 if rubert_score > 0.6 else 0.75

        # 2. üîµ Dynamic –ø–æ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ (0.01s)
        if len(candidates_embs) >= 5:
            sim_matrix = cosine_similarity(candidates_embs[:10])
            redundancy = sim_matrix.mean()
            dynamic_lambda = 1.0 - redundancy * 0.7  # 0.2-0.9
        else:
            dynamic_lambda = 0.5

        # 3. üü£ –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º 40% RuBERT + 60% –¥–∞–Ω–Ω—ã–µ
        final_lambda = 0.4 * rubert_type + 0.6 * dynamic_lambda

        print(f"ü§ñ RuBERT={rubert_type:.2f} Density={dynamic_lambda:.2f} ‚Üí Œª={final_lambda:.2f}")
        return final_lambda

if __name__ == "__main__":
    print(RAGPipeline().query(sys.argv[1]))
